{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67e4daa4",
   "metadata": {},
   "source": [
    "# Q1:\n",
    "Write a python program to scrape data for ‚ÄúData Scientist‚Äù Job position in ‚ÄúBangalore‚Äù location. You have to scrape the job-title, job-location, company_name, experience required. You have to scrape first 10 jobs' data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdd14ef",
   "metadata": {},
   "source": [
    "This task will be done in following steps:\n",
    "\n",
    "1.First, get the webpage https://www.naukri.com/\n",
    "2.Enter ‚ÄúData Scientist‚Äù in ‚ÄúSkill,Designations,Companies‚Äù field and enter ‚ÄúBangalore‚Äù in ‚Äúenter the location‚Äù field.\n",
    "3.Then click the search button.\n",
    "4.Then scrape the data for the first 10 jobs results you get.\n",
    "5.Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "63fb98dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bdf522b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://www.naukri.com\"\n",
    "jobtitle = \"Data Scientist\"\n",
    "location = \"Bangalore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5daea7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "#from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "import requests\n",
    "response = requests.get(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71195b59",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11152/1103105278.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error fetching page\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\\\Users\\\\Sony\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\chromedriver.exe\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "if response.status_code != 200:\n",
    "    print(\"Error fetching page\")\n",
    "    exit()\n",
    "else:\n",
    "    driver = webdriver.Chrome(\"C:\\\\Users\\\\Sony\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\chromedriver.exe\")\n",
    "    driver.get(link)\n",
    "    driver.maximize_window()\n",
    "    #driver.implicitly_wait(10)\n",
    "    \n",
    "    #Enter search conditions\n",
    "    driver.find_element(By.CLASS_NAME,'keywordSugg').find_element(By.CLASS_NAME,'suggestor-input').send_keys(jobtitle)\n",
    "    driver.find_element(By.CLASS_NAME,'locationSugg').find_element(By.CLASS_NAME,'suggestor-input').send_keys(location)\n",
    "    \n",
    "    #click submit/Search button\n",
    "    driver.find_element(By.CLASS_NAME, 'qsbSubmit').click()\n",
    "    #wait for results\n",
    "    \n",
    "    #Creating the DataFrame\n",
    "    df = pd.DataFrame(columns=['Job_Title','Job_Location','Company_Name','Experience_Required'])\n",
    "    \n",
    "    #Extract data:\n",
    "    \n",
    "    #count = int(input())\n",
    "    #Title\n",
    "    job_title=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "    #print(job_title[count].text)\n",
    "    \n",
    "    #Location\n",
    "    job_location = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "    #print(job_location[count].text)\n",
    "\n",
    "    #Company Name\n",
    "    company_name = driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "    #print(company_name[count].text)\n",
    "\n",
    "    #Experience required\n",
    "    exp_required = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']\")\n",
    "    #print(exp_required[count].text)\n",
    "    \n",
    "    for count in range(10):\n",
    "        #Adding to dataframe\n",
    "        df = df.append({'Job_Title':job_title[count].text, 'Job_Location':job_location[count].text,\n",
    "                        'Company_Name':company_name[count].text, \n",
    "                        'Experience_Required':exp_required[count].text},ignore_index = True)\n",
    "        \n",
    "        count = count + 1\n",
    "        if (count==9):\n",
    "            break\n",
    "    driver.close()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721b7115",
   "metadata": {},
   "source": [
    "# Q2:\n",
    "Write a python program to scrape data for ‚ÄúData Scientist‚Äù Job position in ‚ÄúBangalore‚Äù location. You have to scrape the job-title, job-location, company_name, full job-description. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f132a315",
   "metadata": {},
   "source": [
    "This task will be done in following steps:\n",
    "\n",
    "1.First, get the webpage https://www.naukri.com/\n",
    "2.Enter ‚ÄúData Scientist‚Äù in ‚ÄúSkill,Designations,Companies‚Äù field and enter ‚ÄúBangalore‚Äù in ‚Äúenter the location‚Äù field.\n",
    "3.Then click the search button.\n",
    "4.Then scrape the data for the first 10 jobs results you get.\n",
    "5.Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "88d23a45",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12352/1425665617.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0murl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdesc1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"href\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mdriver1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\\\Users\\\\Sony\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\chromedriver.exe\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mdriver1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "if response.status_code != 200:\n",
    "    print(\"Error fetching page\")\n",
    "    exit()\n",
    "else:\n",
    "    driver = webdriver.Chrome(\"C:\\\\Users\\\\Sony\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\chromedriver.exe\")\n",
    "    driver.get(link)\n",
    "    driver.maximize_window()\n",
    "    #driver.implicitly_wait(10)\n",
    "    \n",
    "    #Enter search conditions\n",
    "    driver.find_element(By.CLASS_NAME,'keywordSugg').find_element(By.CLASS_NAME,'suggestor-input').send_keys(jobtitle)\n",
    "    driver.find_element(By.CLASS_NAME,'locationSugg').find_element(By.CLASS_NAME,'suggestor-input').send_keys(location)\n",
    "    \n",
    "    #click submit/Search button\n",
    "    driver.find_element(By.CLASS_NAME, 'qsbSubmit').click()\n",
    "    #wait for results\n",
    "    \n",
    "    #Creating the DataFrame\n",
    "    df = pd.DataFrame(columns=['Job_Title','Job_Location','Company_Name','Job_Description'])\n",
    "    \n",
    "    #Extract data:\n",
    "    #Title\n",
    "    job_title=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "    \n",
    "    #Location\n",
    "    job_location = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "\n",
    "    #Company Name\n",
    "    company_name = driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "    \n",
    "    #Job Description\n",
    "    url=[]\n",
    "    job_desc = []\n",
    "    desc1 = driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "    \n",
    "    for count in range(10):\n",
    "        url.append(desc1[count].get_attribute(\"href\"))\n",
    "        driver1 = webdriver.Chrome(\"C:\\\\Users\\\\Sony\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\chromedriver.exe\")\n",
    "        driver1.get(url[count])\n",
    "        time.sleep(3)\n",
    "        try:\n",
    "            full_desc = driver1.find_element_by_xpath((\"//div[@class='job-desc']\")).text\n",
    "            job_desc.append(full_desc).replace('Job description\\n',' ')\n",
    "        except:\n",
    "            job_desc.append(\"Not Available\") #Appending Not Available for no job description\n",
    "            print(job_desc[count])\n",
    "        \n",
    "        #Adding to dataframe\n",
    "        df = df.append({'Job_Title':job_title[count].text, 'Job_Location':job_location[count].text,\n",
    "                        'Company_Name':company_name[count].text, \n",
    "                        'Job_Description':job_desc[count].text},ignore_index = True)\n",
    "        \n",
    "        count = count + 1\n",
    "        if (count==9):\n",
    "            break\n",
    "    driver.close()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f463ad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fce3afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f66c110b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sony\\AppData\\Local\\Temp/ipykernel_12352/2372367619.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(\"C:\\\\Users\\\\Sony\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\chromedriver.exe\")  #r converts string to raw string\n"
     ]
    }
   ],
   "source": [
    "#Connect to web driver\n",
    "driver=webdriver.Chrome(\"C:\\\\Users\\\\Sony\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\chromedriver.exe\")  #r converts string to raw string\n",
    "#If not r, we can use executable_path = \"C:/path name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8859909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the website to driver\n",
    "driver.get('https://www.naukri.com/')\n",
    "\n",
    "#When we run this line, automatically the webpage will open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d324673c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'driver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11152/1949836187.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Finding the required elements from the search bars of job and location\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mjob_search\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'qsb-keyword-sugg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlocation_search\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'qsb-location-sugg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Sending the inputs to the webpage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mjob_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Data Scientist\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'driver' is not defined"
     ]
    }
   ],
   "source": [
    "#Finding the required elements from the search bars of job and location\n",
    "job_search=driver.find_element('qsb-keyword-sugg')\n",
    "location_search=driver.find_element('qsb-location-sugg')\n",
    "#Sending the inputs to the webpage\n",
    "job_search.send_keys(\"Data Scientist\")\n",
    "location_search.send_keys(\"Bangalore\")\n",
    "#Searching the inputs by using the search button and clicking it\n",
    "driver.find_element_by_xpath(\"//div[@class='search-btn']/button\").click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6981f4a",
   "metadata": {},
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the webpage as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e436d91c",
   "metadata": {},
   "source": [
    "You have to use the location and salary filter.\n",
    "You have to scrape data for ‚ÄúData Scientist‚Äù designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company_name, experience_required.\n",
    "The location filter to be used is ‚ÄúDelhi/NCR‚Äù\n",
    "The salary filter to be used is ‚Äú3-6‚Äù lakhs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f9bdac",
   "metadata": {},
   "source": [
    "The task will be done as shown in the below steps:\n",
    "\n",
    "first get the webpage https://www.naukri.com/\n",
    "Enter ‚ÄúData Scientist‚Äù in ‚ÄúSkill,Designations,Companies‚Äù field.\n",
    "Then click the search button.\n",
    "Then apply the location filter and salary filter by checking the respective boxes\n",
    "Then scrape the data for the first 10 jobs results you get.\n",
    "Finally create a dataframe of the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "17ceb21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to web driver\n",
    "driver=webdriver.Chrome(\"C:\\\\Users\\\\Sony\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\chromedriver.exe\")  #r converts string to raw string\n",
    "#If not r, we can use executable_path = \"C:/path name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ca8c5f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the website to driver\n",
    "driver.get('https://www.naukri.com/')\n",
    "\n",
    "#When we run this line, automatically the webpage will be opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d6b34328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3e8bef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    elem = driver.find_element_by_xpath(\".//*[@id='SORM_TB_ACTION0']\")\n",
    "    elem.click()\n",
    "except NoSuchElementException:  #spelling error making this code not work as expected\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "728cbb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Locating the filters in the webpage\n",
    "#First, we will filter the location\n",
    "filter_location=driver.find_elements_by_xpath(\"//span[@class='ellipsis fleft']\")\n",
    "for i in filter_location:\n",
    "    if i.text=='Delhi / NCR':\n",
    "        i.click()\n",
    "        break   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c9509165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering the salary\n",
    "filter_salary=driver.find_elements_by_xpath(\"//span[@class='ellipsis fleft']\")\n",
    "for i in filter_salary:\n",
    "    if i.text=='3-6 Lakhs':\n",
    "        i.click()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ad0dc293",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After filtering location and salary, we will specify the url of webpage\n",
    "url=\"https://www.naukri.com/data-scientist-jobs?k=data%20scientist&ctcFilter=3to6&cityTypeGid=9508\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3521c45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company name</th>\n",
       "      <th>Experience required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist - Internet Jobs - II</td>\n",
       "      <td>Bangalore/Bengaluru, Delhi / NCR, Mumbai (All ...</td>\n",
       "      <td>Jobs Territory</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida, Bangalore/Bengaluru</td>\n",
       "      <td>Ashkom Media India Private Limited</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Junior Data Scientist</td>\n",
       "      <td>Noida</td>\n",
       "      <td>EASY DATA ANALYTICS TECHNOLOGY PRIVATE LIMITED</td>\n",
       "      <td>1-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist/ Machine Learning, 2022 Passout...</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Chennai, Delhi /...</td>\n",
       "      <td>Creative Hands HR Consultancy</td>\n",
       "      <td>0-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Associate Scientist - Data Engineering</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>AXA Technology Services India Pvt. Ltd</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist || Software Company || Immediat...</td>\n",
       "      <td>Gurgaon/Gurugram, Bangalore/Bengaluru</td>\n",
       "      <td>Skyleaf Consultants</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hiring For Senior Data Scientist-Noida</td>\n",
       "      <td>Noida, Greater Noida, Delhi / NCR</td>\n",
       "      <td>Lumiq.ai</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist (freelance)</td>\n",
       "      <td>New Delhi, Delhi</td>\n",
       "      <td>2Coms</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bharuch, Jaipur, Bhopal, Mumbai, Jhansi, Nagpu...</td>\n",
       "      <td>Country Veggie</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon, Bengaluru</td>\n",
       "      <td>BlackBuck</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                Data Scientist - Internet Jobs - II   \n",
       "1                                     Data Scientist   \n",
       "2                              Junior Data Scientist   \n",
       "3  Data Scientist/ Machine Learning, 2022 Passout...   \n",
       "4             Associate Scientist - Data Engineering   \n",
       "5  Data Scientist || Software Company || Immediat...   \n",
       "6             Hiring For Senior Data Scientist-Noida   \n",
       "7                         Data Scientist (freelance)   \n",
       "8                                     Data Scientist   \n",
       "9                                     Data Scientist   \n",
       "\n",
       "                                            Location  \\\n",
       "0  Bangalore/Bengaluru, Delhi / NCR, Mumbai (All ...   \n",
       "1                         Noida, Bangalore/Bengaluru   \n",
       "2                                              Noida   \n",
       "3  Hyderabad/Secunderabad, Pune, Chennai, Delhi /...   \n",
       "4                                   Gurgaon/Gurugram   \n",
       "5              Gurgaon/Gurugram, Bangalore/Bengaluru   \n",
       "6                  Noida, Greater Noida, Delhi / NCR   \n",
       "7                                   New Delhi, Delhi   \n",
       "8  Bharuch, Jaipur, Bhopal, Mumbai, Jhansi, Nagpu...   \n",
       "9                                 Gurgaon, Bengaluru   \n",
       "\n",
       "                                     Company name Experience required  \n",
       "0                                  Jobs Territory             3-6 Yrs  \n",
       "1              Ashkom Media India Private Limited             3-6 Yrs  \n",
       "2  EASY DATA ANALYTICS TECHNOLOGY PRIVATE LIMITED             1-2 Yrs  \n",
       "3                   Creative Hands HR Consultancy             0-4 Yrs  \n",
       "4          AXA Technology Services India Pvt. Ltd             2-5 Yrs  \n",
       "5                             Skyleaf Consultants             3-8 Yrs  \n",
       "6                                        Lumiq.ai             2-6 Yrs  \n",
       "7                                           2Coms             2-7 Yrs  \n",
       "8                                  Country Veggie             1-3 Yrs  \n",
       "9                                       BlackBuck             3-7 Yrs  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting all the tags having the job title\n",
    "job_title=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "job_title\n",
    "\n",
    "#Extracting the text from the tags\n",
    "title=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in job_title[:10]:\n",
    "    title.append(i.text)\n",
    "title   \n",
    "\n",
    "#Extracting all the tags having the job location\n",
    "job_location=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span\")\n",
    "job_location\n",
    "\n",
    "#Extracting the text from the tags\n",
    "location=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in job_location[:10]:\n",
    "    location.append(i.text)\n",
    "location    \n",
    "\n",
    "#Extracting the tags having the company name\n",
    "name=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "name\n",
    "\n",
    "#Extracting the text from the tags\n",
    "company=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in name[:10]:\n",
    "    company.append(i.text)\n",
    "company\n",
    "\n",
    "#Extracting the tags having experience required\n",
    "exp_required=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span\")\n",
    "exp_required\n",
    "\n",
    "#Extracting the text from the tags\n",
    "exp=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in exp_required[:10]:\n",
    "    exp.append(i.text)\n",
    "exp\n",
    "\n",
    "#Checking out the length of the data extracted\n",
    "print(len(title),len(location),len(company),len(exp))\n",
    "\n",
    "#Creating a new dataframe for saving the data\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['Title']=title\n",
    "jobs['Location']=location\n",
    "jobs['Company name']=company\n",
    "jobs['Experience required']=exp\n",
    "jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f222a2b",
   "metadata": {},
   "source": [
    "Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes: 1. Brand 2. Product Description 3. Price The attributes which you have to scrape is ticked marked in the below image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f4bbb",
   "metadata": {},
   "source": [
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/ \n",
    "2. Enter ‚Äúsunglasses‚Äù in the search field where ‚Äúsearch for products, brands andmore‚Äù is written and click the search icon \n",
    "3. After that you will reach to the page having a lot of sunglasses. From this pageyou can scrap the required data as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "38beb50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to web driver\n",
    "driver=webdriver.Chrome(\"C:\\\\Users\\\\Sony\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\chromedriver.exe\")  #r converts string to raw string\n",
    "#If not r, we can use executable_path = \"C:/path name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "93a5c7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the website to driver\n",
    "driver.get(' https://www.flipkart.com/')\n",
    "\n",
    "#When we run this line, automatically the webpage will be opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "01ccfbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying the url of the webpage to be scraped\n",
    "url=\"https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "666b2d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking the empty lists\n",
    "brand=[]\n",
    "prod_desc=[]\n",
    "org_price=[]\n",
    "discount=[]\n",
    "disc_price=[]\n",
    "\n",
    "#We need to scrap for 100 products and here we will consider the data from first 3 pages\n",
    "#We will take a for loop and scrap data from all the pages\n",
    "for i in range(0,3):\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\"):\n",
    "        brand.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\"):\n",
    "        prod_desc.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_3I9_wc']\"):\n",
    "        org_price.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']/span\"):\n",
    "        discount.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\"):\n",
    "        disc_price.append(j.text)\n",
    "   \n",
    "    #Path for next page as it changes for every page. We are appending numbers as pages change  \n",
    "    k=i+1\n",
    "    next_page=\"https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page=\"+str(k)\n",
    "    driver.get(next_page)\n",
    "    time.sleep(2)  #2 seconds wait time for each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5eebe725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 118 120 120 120\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Description</th>\n",
       "      <th>Original Price</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Discounted Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Roadster</td>\n",
       "      <td>UV Protection Cat-eye Sunglasses (54)</td>\n",
       "      <td>‚Çπ1,199</td>\n",
       "      <td>39% off</td>\n",
       "      <td>‚Çπ729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>‚Çπ1,999</td>\n",
       "      <td>70% off</td>\n",
       "      <td>‚Çπ599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection, Polarized Wayfarer Sunglasses (...</td>\n",
       "      <td>‚Çπ799</td>\n",
       "      <td>26% off</td>\n",
       "      <td>‚Çπ589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SUNBEE</td>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>‚Çπ1,299</td>\n",
       "      <td>82% off</td>\n",
       "      <td>‚Çπ233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>Riding Glasses Wrap-around Sunglasses (Free Size)</td>\n",
       "      <td>‚Çπ1,599</td>\n",
       "      <td>87% off</td>\n",
       "      <td>‚Çπ199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>PHENOMENAL</td>\n",
       "      <td>UV Protection, Night Vision Wayfarer Sunglasse...</td>\n",
       "      <td>‚Çπ1,999</td>\n",
       "      <td>84% off</td>\n",
       "      <td>‚Çπ303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>UV Protection, Mirrored, Gradient Round Sungla...</td>\n",
       "      <td>‚Çπ999</td>\n",
       "      <td>82% off</td>\n",
       "      <td>‚Çπ179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>kingsunglasses</td>\n",
       "      <td>Polarized Aviator Sunglasses (Free Size)</td>\n",
       "      <td>‚Çπ1,699</td>\n",
       "      <td>85% off</td>\n",
       "      <td>‚Çπ249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Rich Club</td>\n",
       "      <td>Polarized, Riding Glasses, Night Vision Sports...</td>\n",
       "      <td>‚Çπ799</td>\n",
       "      <td>78% off</td>\n",
       "      <td>‚Çπ173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Mi</td>\n",
       "      <td>UV Protection, Riding Glasses Rectangular, Ret...</td>\n",
       "      <td>‚Çπ1,199</td>\n",
       "      <td>20% off</td>\n",
       "      <td>‚Çπ959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Brand                                        Description  \\\n",
       "0         Roadster              UV Protection Cat-eye Sunglasses (54)   \n",
       "1    VINCENT CHASE   UV Protection Rectangular Sunglasses (Free Size)   \n",
       "2         Fastrack  UV Protection, Polarized Wayfarer Sunglasses (...   \n",
       "3           SUNBEE              UV Protection Aviator Sunglasses (54)   \n",
       "4           PIRASO  Riding Glasses Wrap-around Sunglasses (Free Size)   \n",
       "..             ...                                                ...   \n",
       "95      PHENOMENAL  UV Protection, Night Vision Wayfarer Sunglasse...   \n",
       "96       ROYAL SON  UV Protection, Mirrored, Gradient Round Sungla...   \n",
       "97  kingsunglasses           Polarized Aviator Sunglasses (Free Size)   \n",
       "98       Rich Club  Polarized, Riding Glasses, Night Vision Sports...   \n",
       "99              Mi  UV Protection, Riding Glasses Rectangular, Ret...   \n",
       "\n",
       "   Original Price Discount Discounted Price  \n",
       "0          ‚Çπ1,199  39% off             ‚Çπ729  \n",
       "1          ‚Çπ1,999  70% off             ‚Çπ599  \n",
       "2            ‚Çπ799  26% off             ‚Çπ589  \n",
       "3          ‚Çπ1,299  82% off             ‚Çπ233  \n",
       "4          ‚Çπ1,599  87% off             ‚Çπ199  \n",
       "..            ...      ...              ...  \n",
       "95         ‚Çπ1,999  84% off             ‚Çπ303  \n",
       "96           ‚Çπ999  82% off             ‚Çπ179  \n",
       "97         ‚Çπ1,699  85% off             ‚Çπ249  \n",
       "98           ‚Çπ799  78% off             ‚Çπ173  \n",
       "99         ‚Çπ1,199  20% off             ‚Çπ959  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the length of the data scraped\n",
    "print(len(brand),len(prod_desc),len(org_price),len(discount),len(disc_price))\n",
    "\n",
    "#Creating a new dataframe\n",
    "glass=pd.DataFrame({})\n",
    "glass['Brand']=brand[:100]\n",
    "glass['Description']=prod_desc[:100]\n",
    "glass['Original Price']=org_price[:100]\n",
    "glass['Discount']=discount[:100]\n",
    "glass['Discounted Price']=disc_price[:100]\n",
    "glass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0500bfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f6eca6",
   "metadata": {},
   "source": [
    "Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f06f3d",
   "metadata": {},
   "source": [
    "You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace\n",
    "When you will open the above link you will reach to the webpage.\n",
    "As shown in the above page you have to scrape the following attributes:\n",
    "Rating\n",
    "Review_summary\n",
    "Full review\n",
    "You have to scrape this data for first 100 reviews.\n",
    "Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3cc6ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to web driver\n",
    "driver=webdriver.Chrome(\"C:\\\\Users\\\\Sony\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\chromedriver.exe\")  #r converts string to raw string\n",
    "#If not r, we can use executable_path = \"C:/path name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "eb45d794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the website to driver\n",
    "driver.get('https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART')\n",
    "\n",
    "#When we run this line, automatically the webpage will be opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6e090309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 110 110\n"
     ]
    }
   ],
   "source": [
    "#Taking the empty lists\n",
    "Rating=[]\n",
    "review=[]\n",
    "full_summary=[]\n",
    "\n",
    "#As there are nearly 10 reviews per page, we will check for 11 pages and scrap the required data\n",
    "#Now we will take a for loop and scrap\n",
    "for i in range(0,11):\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\"):\n",
    "        Rating.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\"):\n",
    "        review.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\"):\n",
    "        full_summary.append(j.text)\n",
    "        \n",
    "    #Path for next page as it changes for every page. We are appending numbers as pages change  \n",
    "    k=i+1\n",
    "    next_page=\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=\"+str(k) \n",
    "    driver.get(next_page)\n",
    "    time.sleep(2)  #2 seconds wait time for each page\n",
    "    \n",
    "#Checking the length of the data scraped\n",
    "print(len(Rating),len(review),len(full_summary))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "455bdb8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>Full summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>The Best Phone for the Money\\n\\nThe iPhone 11 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Really satisfied with the Product I received.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Great iPhone very snappy experience as apple k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>This is my first iOS phone. I am very happy wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Super!</td>\n",
       "      <td>Did an upgrade from 6s plus to iphone 11.\\nAo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>4</td>\n",
       "      <td>Classy product</td>\n",
       "      <td>Gifted my man on his 30th birthday üéÇ He loves ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Here is the thing\\n\\nThe only reason why you s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>It was amazing experience for me. Honestly i a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Nice product</td>\n",
       "      <td>If you are looking for a premium phone under 5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating               Review  \\\n",
       "0       5            Brilliant   \n",
       "1       5       Simply awesome   \n",
       "2       5  Best in the market!   \n",
       "3       5     Perfect product!   \n",
       "4       5            Fabulous!   \n",
       "..    ...                  ...   \n",
       "95      5               Super!   \n",
       "96      4       Classy product   \n",
       "97      5    Worth every penny   \n",
       "98      5            Excellent   \n",
       "99      5         Nice product   \n",
       "\n",
       "                                         Full summary  \n",
       "0   The Best Phone for the Money\\n\\nThe iPhone 11 ...  \n",
       "1   Really satisfied with the Product I received.....  \n",
       "2   Great iPhone very snappy experience as apple k...  \n",
       "3   Amazing phone with great cameras and better ba...  \n",
       "4   This is my first iOS phone. I am very happy wi...  \n",
       "..                                                ...  \n",
       "95  Did an upgrade from 6s plus to iphone 11.\\nAo ...  \n",
       "96  Gifted my man on his 30th birthday üéÇ He loves ...  \n",
       "97  Here is the thing\\n\\nThe only reason why you s...  \n",
       "98  It was amazing experience for me. Honestly i a...  \n",
       "99  If you are looking for a premium phone under 5...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a new dataframe\n",
    "phone=pd.DataFrame({})\n",
    "phone['Rating']=Rating[:100]\n",
    "phone['Review']=review[:100]\n",
    "phone['Full summary']=full_summary[:100]\n",
    "phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a7135842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5929097e",
   "metadata": {},
   "source": [
    "Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com andsearch for ‚Äúsneakers‚Äù in the search field. You have to scrape 4 attributes of each sneaker: \n",
    "1. Brand \n",
    "2. Product Description\n",
    "3. Price As shown in the below image, you have to scrape the tick marked attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "25ed2243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to web driver\n",
    "driver=webdriver.Chrome(\"C:\\\\Users\\\\Sony\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\chromedriver.exe\")  #r converts string to raw string\n",
    "#If not r, we can use executable_path = \"C:/path name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b26c85f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the website to driver\n",
    "driver.get(' https://www.flipkart.com/')\n",
    "\n",
    "#When we run this line, automatically the webpage will be opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ab009de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying the url of the webpage to be scraped\n",
    "url=\"https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3d9e851f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 122 160 160 160\n"
     ]
    }
   ],
   "source": [
    "#Taking the empty lists\n",
    "brand=[]\n",
    "prod_desc=[]\n",
    "org_price=[]\n",
    "discount=[]\n",
    "disc_price=[]\n",
    "\n",
    "#We need to scrap for 100 products and here we will consider the data from first 4 pages\n",
    "#We will take a for loop and scrap data from all the pages\n",
    "for i in range(0,4):\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\"):\n",
    "        brand.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\"):\n",
    "        prod_desc.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_3I9_wc']\"):\n",
    "        org_price.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']/span\"):\n",
    "        discount.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\"):\n",
    "        disc_price.append(j.text)\n",
    "   \n",
    "    #Path for next page as it changes for every page. We are appending numbers as pages change  \n",
    "    k=i+1\n",
    "    next_page=\"https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page=\"+str(k)\n",
    "    driver.get(next_page)\n",
    "    time.sleep(2)  #2 seconds wait time for each page\n",
    "    \n",
    "#Checking the length of the data scraped\n",
    "print(len(brand),len(prod_desc),len(org_price),len(discount),len(disc_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7fad8982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Description</th>\n",
       "      <th>Original Price</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Discounted Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kirjak</td>\n",
       "      <td>Modern Trendy Sneakers Shoes Sneakers For Men</td>\n",
       "      <td>‚Çπ1,999</td>\n",
       "      <td>72% off</td>\n",
       "      <td>‚Çπ549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RED TAPE</td>\n",
       "      <td>STYLISH MENS BLACK AND WHITE SNEAKER Sneakers ...</td>\n",
       "      <td>‚Çπ4,699</td>\n",
       "      <td>76% off</td>\n",
       "      <td>‚Çπ1,124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>URBANBOX</td>\n",
       "      <td>Kwik FIT casual sneaker shoes and partywear sh...</td>\n",
       "      <td>‚Çπ999</td>\n",
       "      <td>85% off</td>\n",
       "      <td>‚Çπ148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RapidBox</td>\n",
       "      <td>Stylish Comfortable Lightweight, Breathable Wa...</td>\n",
       "      <td>‚Çπ999</td>\n",
       "      <td>39% off</td>\n",
       "      <td>‚Çπ600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>Sneaker Sneakers For Men</td>\n",
       "      <td>‚Çπ1,299</td>\n",
       "      <td>81% off</td>\n",
       "      <td>‚Çπ245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>RED TAPE</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>‚Çπ4,699</td>\n",
       "      <td>76% off</td>\n",
       "      <td>‚Çπ1,124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>LE GREEM</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>‚Çπ999</td>\n",
       "      <td>55% off</td>\n",
       "      <td>‚Çπ449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>T-ROCK</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>‚Çπ999</td>\n",
       "      <td>75% off</td>\n",
       "      <td>‚Çπ249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Dizzler</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>‚Çπ719</td>\n",
       "      <td>38% off</td>\n",
       "      <td>‚Çπ445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>RED TAPE</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>‚Çπ4,195</td>\n",
       "      <td>76% off</td>\n",
       "      <td>‚Çπ998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Brand                                        Description  \\\n",
       "0     Kirjak      Modern Trendy Sneakers Shoes Sneakers For Men   \n",
       "1   RED TAPE  STYLISH MENS BLACK AND WHITE SNEAKER Sneakers ...   \n",
       "2   URBANBOX  Kwik FIT casual sneaker shoes and partywear sh...   \n",
       "3   RapidBox  Stylish Comfortable Lightweight, Breathable Wa...   \n",
       "4     BRUTON                           Sneaker Sneakers For Men   \n",
       "..       ...                                                ...   \n",
       "95  RED TAPE                                   Sneakers For Men   \n",
       "96  LE GREEM                                   Sneakers For Men   \n",
       "97    T-ROCK                                   Sneakers For Men   \n",
       "98   Dizzler                                   Sneakers For Men   \n",
       "99  RED TAPE                                   Sneakers For Men   \n",
       "\n",
       "   Original Price Discount Discounted Price  \n",
       "0          ‚Çπ1,999  72% off             ‚Çπ549  \n",
       "1          ‚Çπ4,699  76% off           ‚Çπ1,124  \n",
       "2            ‚Çπ999  85% off             ‚Çπ148  \n",
       "3            ‚Çπ999  39% off             ‚Çπ600  \n",
       "4          ‚Çπ1,299  81% off             ‚Çπ245  \n",
       "..            ...      ...              ...  \n",
       "95         ‚Çπ4,699  76% off           ‚Çπ1,124  \n",
       "96           ‚Çπ999  55% off             ‚Çπ449  \n",
       "97           ‚Çπ999  75% off             ‚Çπ249  \n",
       "98           ‚Çπ719  38% off             ‚Çπ445  \n",
       "99         ‚Çπ4,195  76% off             ‚Çπ998  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a new dataframe\n",
    "sneakers=pd.DataFrame({})\n",
    "sneakers['Brand']=brand[:100]\n",
    "sneakers['Description']=prod_desc[:100]\n",
    "sneakers['Original Price']=org_price[:100]\n",
    "sneakers['Discount']=discount[:100]\n",
    "sneakers['Discounted Price']=disc_price[:100]\n",
    "sneakers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15efd0e1",
   "metadata": {},
   "source": [
    "Q8: Go to webpage https://www.amazon.in/ Enter ‚ÄúLaptop‚Äù in the search field and then click the search icon. Then set CPU Type filter to ‚ÄúIntel Core i7‚Äù and ‚ÄúIntel Core i9‚Äù as shown in the below image:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09503ef9",
   "metadata": {},
   "source": [
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop: \n",
    "1. Title \n",
    "2. Ratings \n",
    "3. Price As shown in the below image as the tick marked attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9b5e37e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing exception\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "507a0632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to web driver\n",
    "driver=webdriver.Chrome(\"C:\\\\Users\\\\Sony\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\chromedriver.exe\")  #r converts string to raw string\n",
    "#If not r, we can use executable_path = \"C:/path name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b2567e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the website to driver\n",
    "driver.get('https://www.amazon.in/')\n",
    "\n",
    "#When we run this line, automatically the webpage will be opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a93882e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Searching laptop in the search bar and clicking the search button\n",
    "search_bar=driver.find_element_by_id('twotabsearchtextbox')\n",
    "search_bar.send_keys(\"laptop\")\n",
    "\n",
    "driver.find_element_by_id('nav-search-submit-button').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2d27047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Locating the filters from the webpage\n",
    "#Filtering Intel Core i7 from filters\n",
    "filter1=driver.find_elements_by_xpath(\"//a[@class='a-link-normal s-navigation-item']/span\")\n",
    "for i in filter1:\n",
    "    if i.text=='Intel Core i7':\n",
    "        i.click()\n",
    "        break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d03de195",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering Intel Core i9 from filters\n",
    "filter1=driver.find_elements_by_xpath(\"//a[@class='a-link-normal s-navigation-item']/span\")\n",
    "for i in filter1:\n",
    "    if i.text=='Intel Core i9':\n",
    "        i.click()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d27d7ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying the url of the webpage to be scraped\n",
    "url=\"https://www.amazon.in/s?k=laptop&i=computers&rh=n%3A1375424031%2Cp_n_feature_thirteen_browse-bin%3A12598163031%7C16757432031&dc&qid=1617789939&rnid=12598141031&ref=sr_nr_p_n_feature_thirteen_browse-bin_17\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "3f120cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 0\n"
     ]
    }
   ],
   "source": [
    "#Extracting the tags having the product title\n",
    "title=driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "title\n",
    "\n",
    "#Extracting the text from the tags\n",
    "prod_title=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 product results, we are running a for loop for first 10 results only\n",
    "for i in title[:10]:\n",
    "    prod_title.append(i.text)\n",
    "prod_title    \n",
    "\n",
    "#Extracting the tags having the price of the product\n",
    "price=driver.find_elements_by_xpath(\"//span[@class='a-price']\")\n",
    "price\n",
    "\n",
    "#Extracting the text from the tags\n",
    "prod_price=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 product results, we are running a for loop for first 10 results only\n",
    "for i in price[:10]:\n",
    "    prod_price.append(i.text)\n",
    "prod_price\n",
    "\n",
    "#Extracting the tags having the product ratings\n",
    "#First we will collect the urls of all laptops\n",
    "laptop_urls=driver.find_elements_by_xpath(\"//a[@class='a-link-normal a-text-normal']\")\n",
    "URL=[]   #Taking an empty list\n",
    "\n",
    "#Appending the url of first 10 laptops to empty list\n",
    "for i in laptop_urls[:10]:\n",
    "    URL.append(i.get_attribute('href'))   #Getting url alone\n",
    "URL    \n",
    "\n",
    "#Extracting the ratings of the laptop by using exception as some products dont have any ratings\n",
    "Ratings=[]   #Empty list\n",
    "#Loop for every laptops in the list\n",
    "for url in URL:\n",
    "    driver.get(url)\n",
    "    try:   #Exception handling by using NoSuchElementException\n",
    "        prod_rating=driver.find_element_by_id('acrCustomerReviewText')  #Locating the rating link\n",
    "        prod_rating.click()\n",
    "        rating=driver.find_element_by_xpath(\"//span[@class='a-size-medium a-color-base']\") #Locating the rating tags\n",
    "        Ratings.append(rating.text)  #Appending the text from tags to the list\n",
    "    except NoSuchElementException as e:\n",
    "        Ratings.append('No Rating')  #Appending message for products having no ratings\n",
    "        \n",
    "#Checking out the length of the data extracted\n",
    "print(len(prod_title),len(prod_price),len(Ratings))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "327bc7dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (0) does not match length of index (10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12352/166527992.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mamazon\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Product name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprod_title\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mamazon\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Price'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprod_price\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mamazon\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Rating'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRatings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mamazon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3610\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3611\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3612\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3614\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3782\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3783\u001b[0m         \"\"\"\n\u001b[1;32m-> 3784\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3786\u001b[0m         if (\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4508\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4509\u001b[1;33m             \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4510\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \"\"\"\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    532\u001b[0m             \u001b[1;34m\"Length of values \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m             \u001b[1;34mf\"({len(data)}) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (0) does not match length of index (10)"
     ]
    }
   ],
   "source": [
    "#Creating a new dataframe for saving the data\n",
    "amazon=pd.DataFrame({})\n",
    "amazon['Product name']=prod_title\n",
    "amazon['Price']=prod_price\n",
    "amazon['Rating']=Ratings\n",
    "amazon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d7929b",
   "metadata": {},
   "source": [
    "Q9: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida location. You have to scrape company name, No. of days ago when job was posted, Rating of the company. This task will be done in following steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the Job option as shown in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "bd23497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to web driver\n",
    "driver=webdriver.Chrome(\"C:\\\\Users\\\\Sony\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\chromedriver.exe\")  #r converts string to raw string\n",
    "#If not r, we can use executable_path = \"C:/path name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f94ca1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the website to driver\n",
    "driver.get('https://www.ambitionbox.com/.in/Salaries/index.htm')\n",
    "\n",
    "#When we run this line, automatically the webpage will be opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50039227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6782c1a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'webdriver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6272/2372367619.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Connect to web driver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdriver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\\\Users\\\\Sony\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\chromedriver.exe\"\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#r converts string to raw string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#If not r, we can use executable_path = \"C:/path name\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'webdriver' is not defined"
     ]
    }
   ],
   "source": [
    "#Connect to web driver\n",
    "driver=webdriver.Chrome(\"C:\\\\Users\\\\Sony\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\chromedriver.exe\")  #r converts string to raw string\n",
    "#If not r, we can use executable_path = \"C:/path name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "276f5b78",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'driver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6272/1373535130.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Finding the required elements from the search bars of job and location\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mjob_search\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'KeywordSearch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlocation_search\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'LocationSearch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0muserID\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".//*[@id='UserName']\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0muserID\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'username'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'driver' is not defined"
     ]
    }
   ],
   "source": [
    "#Finding the required elements from the search bars of job and location\n",
    "job_search=driver.find_element_by_id('KeywordSearch')\n",
    "location_search=driver.find_element_by_id('LocationSearch')\n",
    "userID = driver.find_element_by_xpath(\".//*[@id='UserName']\")\n",
    "userID.send_keys('username')\n",
    "\n",
    "#Sending the inputs to the webpage\n",
    "job_search.send_keys(\"Data Scientist\")\n",
    "location_search.send_keys(\"Noida\")\n",
    "\n",
    "#Searching the inputs by using the search button and clicking it\n",
    "driver.find_element_by_xpath(\"//button[@class='gd-btn-mkt']\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "252cfe21",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'driver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6272/17293377.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Specifying the url of the webpage to be scraped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"https://www.ambitionbox.com/.in/Salaries/new-delhi-data-scientist-salary-SRCH_IL.0,9_IM1083_KO10,24.htm?clickSource=searchBtn\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'driver' is not defined"
     ]
    }
   ],
   "source": [
    "#Specifying the url of the webpage to be scraped\n",
    "url=\"https://www.ambitionbox.com/.in/Salaries/new-delhi-data-scientist-salary-SRCH_IL.0,9_IM1083_KO10,24.htm?clickSource=searchBtn\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0c8de6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'driver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6272/2113264500.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Extracting the tags having the company name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcomp_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_elements_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"//div[@class='d-flex']/div[2]/p[2]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcomp_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#Extracting the text from the tags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'driver' is not defined"
     ]
    }
   ],
   "source": [
    "#Extracting the tags having the company name\n",
    "comp_name=driver.find_elements_by_xpath(\"//div[@class='d-flex']/div[2]/p[2]\")\n",
    "comp_name\n",
    "\n",
    "#Extracting the text from the tags\n",
    "company=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in comp_name[:10]:\n",
    "    company.append(i.text)\n",
    "company\n",
    "\n",
    "#Extracting the tags having the number of salaries\n",
    "no_salaries=driver.find_elements_by_xpath(\"//div[@class='d-flex']/div[2]/p[5]\")\n",
    "no_salaries\n",
    "\n",
    "#Extracting the text from the tags\n",
    "no_of_salaries=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in no_salaries[:10]:\n",
    "    no_of_salaries.append(i.text)\n",
    "no_of_salaries\n",
    "\n",
    "#Extracting the tags having average salary\n",
    "avg_sal=driver.find_elements_by_xpath(\"//div[@class='col-2 d-none d-md-flex flex-row justify-content-end']\")\n",
    "avg_sal\n",
    "\n",
    "#Extracting the text from the tags\n",
    "avg_salary=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in avg_sal[:10]:\n",
    "    avg_salary.append(i.text.replace('\\n',''))\n",
    "avg_salary\n",
    "\n",
    "#Extracting the tags having minimum salary\n",
    "min_sal=driver.find_elements_by_xpath(\"//div[@class='col-3 offset-1 d-none d-md-block']/div/div[2]/span[1]\")\n",
    "min_sal\n",
    "\n",
    "#Extracting the text from the tags\n",
    "min_salary=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in min_sal[:10]:\n",
    "    min_salary.append(i.text)\n",
    "min_salary\n",
    "\n",
    "#Extracting the tags having the maximum salary\n",
    "max_sal=driver.find_elements_by_xpath(\"//div[@class='col-3 offset-1 d-none d-md-block']/div/div[2]/span[2]\")\n",
    "max_sal\n",
    "\n",
    "#Extracting the text from the tags\n",
    "max_salary=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in max_sal[:10]:\n",
    "    max_salary.append(i.text)\n",
    "max_salary\n",
    "\n",
    "#Checking out the length of the data extracted\n",
    "print(len(company),len(no_of_salaries),len(avg_salary),len(min_salary),len(max_salary))\n",
    "\n",
    "#Creating a new dataframe for saving the data\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['Company Name']=company\n",
    "jobs['Number of salaries']=no_of_salaries\n",
    "jobs['Average salary']=avg_salary\n",
    "jobs['Minimum salary']=min_salary\n",
    "jobs['Maximum salary']=max_salary\n",
    "jobs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
